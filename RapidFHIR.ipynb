{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, the dependencies.  Yu have to have Pytorch installed, and transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Check if GPU is available\n",
    "print(torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pretrained T5 model and tokenizer\n",
    "# Replace 'path/to/model' with the actual path to the directory containing the .bin file, or you can just use our huggingface version\n",
    "# Visit https://huggingface.co/fhirfly/rapidfhir-procedures for more information on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "\n",
    "model_path = 'fhirfly/rapidfhir-procedures'\n",
    "\n",
    "# Load the model from the .bin file\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: [resourceType] Procedure [id] efffddd8-effa-effa-ffaa-ffaffffffff [meta][profile][0] http://hl7.org/fhir/us/core/StructureDefinition/us-core-procedure [status] completed [code][coding][0][system] http://snomed.info/sct [code][coding][0][code] 430193006 [code][coding][0][display] Medication Reconciliation (procedure) [code][text] Medication Reconciliation (procedure) [subject][reference] Patient/fffffffd-fffa-fffa-fffa-fffffffffff [encounter][reference] Encounter/ffffffff-fffa-fffa-fffa-fffffffffff [performedPeriod][start] 2020-03-09T11:38:21-05:00 [performedPeriod][end] 2020-03-09T11:36:21-05:00 [location][reference] Location?identifier=https://github.com/synthetichealth/synthea|fffd0bf3-ffaa-3efd-affa-fffdfffffff [location][display] afffd0d-faed-bffa-fffa-fffffffffff [location][display] PCP237a3-faed-ffaa-ffffffffff [location][reference] Location?identifier=https://github.com/synthea|fff0bff9-ffaa-dfdd-ffc5-ffffffffff [location][display] PCP23757\n"
     ]
    }
   ],
   "source": [
    "# Prompt for text generation\n",
    "prompt = \"SQM9PZ2545XHC4TE9RS27V183DD9KPW6JOI53UU5NYY8XRGIW6NZ0227WOAAW6NDNO79SR2K75T6J104XSAKMITKD8B8GPHGLQY424SHKI8OKQXXQN8BG435OKAMLFEN\"\n",
    "\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Generate text with a maximum length of 4096 tokens\n",
    "max_length = 4096\n",
    "output = model.generate(input_ids, max_length=max_length)\n",
    "# Move the output tensor back to CPU and decode the generated output\n",
    "output = output.to(\"cpu\")\n",
    "# Decode the generated output\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Summary:\", generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
